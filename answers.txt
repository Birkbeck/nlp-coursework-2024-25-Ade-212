1d. The Flesh Kincaid score is useful for gauging general readability but it can be unreliable when its applied to texts that fall outside its narrow focus on sentence length and syllable counts. For example, highly technical or domain-specific writings like legal documents, scientific articles, or medical reports tends to rely on precise terminology and complex concepts or equations that the formula cannot capture. Despite short sentences or familiar words, the specific terminology may remain difficult to understand for non-experts, especially within the appropriate context. Likewise, very short excerpts or non-prose genres such as poetry, dialogue-heavy scripts or even computer code tend to undermine its reliability. A single long sentence or some polysyllabic words can skew the score, and unconventional structure means the algorithm simply doesn’t map onto the way these texts are meant to be read. In both cases, Flesch Kincaid stops being a valid proxy for true difficulty.
2f. The custom tokenizer function first runs each speech through a lightweight spaCy pipeline (tagger and lemmatiser only). For every token it keeps only alphabetic strings, drops English stop-words, removes tokens shorter than three characters, and returns the lower-cased lemma. This sequence of filters compresses inflected variants into a single form (e.g. run, running, ran to run), discards punctuation and numbers that carry little party information, and weeds out high-frequency function words that would consume part of the 3000 feature limit. Because of this the TF-IDF vocabulary is filled with more content-rich lemmas that are better at signalling ideological differences. When the speeches are vectorised with this tokenizer (that is still capped at 3000 features) and fed to the same Random-Forest and linear-kernel SVM models, the SVM achieves a macro-average F1 score of 0.77, which essentially matches the unigram baseline and only a couple of points below the best 1–3 gram system—while training faster and with a cleaner, smaller feature space. Thus, the tokenizer offers a good trade-off as it simplifies the input, keeps computation light, and delivers competitive classification performance without exceeding the feature limit.